# InferBench-Framework

**Unified Benchmarking Framework for AI Factory Workloads**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

## ğŸ¯ Project Overview

InferBench-Framework is a modular benchmarking framework designed to evaluate the performance of AI Factory components on the **MeluXina supercomputer**. This project is part of the **EUMaster4HPC Student Challenge 2025-2026**.

The framework provides tools for:
- ğŸ–¥ï¸ **Server Management**: Deploy and manage AI services (vLLM, Ollama, Vector DBs)
- ğŸ“Š **Client Workloads**: Generate benchmark loads against services
- ğŸ“ˆ **Monitoring**: Real-time metrics collection with Prometheus + Grafana
- ğŸ“ **Logging**: Centralized log collection and analysis
- ğŸ›ï¸ **Interface**: CLI and Web UI for orchestration

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Interface (CLI/Web)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Servers   â”‚   Clients   â”‚  Monitors   â”‚       Logs       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Core Infrastructure                       â”‚
â”‚              (Recipes, Registry, Orchestrator)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  SLURM / Apptainer Runtime                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10 or higher
- [Poetry](https://python-poetry.org/docs/#installation) 2.0+ for dependency management
- Access to MeluXina supercomputer (for HPC features)

> **Note for Poetry 2.0+**: The `poetry shell` command is no longer available by default. Use `eval $(poetry env activate)` or prefix commands with `poetry run`.

### Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/moa155/InferBench-Framework.git
   cd InferBench-Framework
   ```

2. **Install dependencies**:
   ```bash
   poetry install
   ```

3. **Activate the virtual environment**:
   ```bash
   # Poetry 2.0+ (recommended)
   eval $(poetry env activate)
   
   # Or use poetry run for individual commands
   poetry run inferbench --help
   ```

4. **Verify installation**:
   ```bash
   inferbench --version
   inferbench --help
   ```

## ğŸš€ Quick Start

### Start a Server
```bash
# List available server recipes
inferbench server list

# Start a vLLM inference server
inferbench server start --recipe vllm-inference
```

### Run a Benchmark Client
```bash
# Run a stress test against the server
inferbench client run --recipe llm-stress-test
```

### Monitor Services
```bash
# Start monitoring stack (Prometheus + Grafana)
inferbench monitor start --recipe default-monitor --targets <job-id>
```

### View Logs
```bash
# Get logs for a specific service
inferbench logs show --service-id <service-id>
```

## ğŸ“ Project Structure

```
InferBench-Framework/
â”œâ”€â”€ src/inferbench/           # Main source code
â”‚   â”œâ”€â”€ servers/            # Server module
â”‚   â”œâ”€â”€ clients/            # Client module
â”‚   â”œâ”€â”€ monitors/           # Monitor module
â”‚   â”œâ”€â”€ logs/               # Logs module
â”‚   â”œâ”€â”€ interface/          # CLI and Web interface
â”‚   â”œâ”€â”€ core/               # Core infrastructure
â”‚   â””â”€â”€ utils/              # Utility functions
â”œâ”€â”€ recipes/                # YAML recipe configurations
â”‚   â”œâ”€â”€ servers/            # Server recipes
â”‚   â”œâ”€â”€ clients/            # Client recipes
â”‚   â”œâ”€â”€ monitors/           # Monitor recipes
â”‚   â””â”€â”€ benchmarks/         # Full benchmark recipes
â”œâ”€â”€ config/                 # Configuration files
â”œâ”€â”€ tests/                  # Test suite
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ templates/              # Report and dashboard templates
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ results/                # Benchmark results
â””â”€â”€ examples/               # Example configurations
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# MeluXina Configuration
MELUXINA_USER=your_username
MELUXINA_PROJECT=your_project

# Framework Settings
INFERBENCH_LOG_LEVEL=INFO
INFERBENCH_CONFIG_DIR=/path/to/configs
INFERBENCH_RESULTS_DIR=/path/to/results

# Monitoring
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
```

### Recipe Configuration

Recipes are YAML files that define services, clients, and monitoring:

```yaml
# recipes/servers/vllm-inference.yaml
name: vllm-inference
type: server
image: /path/to/vllm.sif
resources:
  nodes: 1
  gpus: 1
  memory: 32G
  time: "02:00:00"
ports:
  - 8000
environment:
  MODEL_NAME: "meta-llama/Llama-2-7b-hf"
healthcheck:
  endpoint: /health
  interval: 30s
```

## ğŸ“Š Monitoring

The framework integrates with Prometheus and Grafana for real-time monitoring:

- **Prometheus**: Metrics collection and storage
- **Grafana**: Visualization dashboards
- **Custom Exporters**: Service-specific metrics

## ğŸ§ª Testing

```bash
# Run all tests
poetry run pytest

# Run with coverage
poetry run pytest --cov=src/inferbench

# Run specific test file
poetry run pytest tests/unit/test_servers.py
```

## ğŸ“ Development

### Code Formatting
```bash
poetry run black src/
poetry run ruff check src/
```

### Type Checking
```bash
poetry run mypy src/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- EUMaster4HPC Program
- LuxProvide and MeluXina Supercomputer
- Dr. Farouk Mansouri for supervision and mentoring

---

**Built with â¤ï¸ for the EUMaster4HPC Student Challenge 2025-2026**
