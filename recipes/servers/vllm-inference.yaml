# InferBench Server Recipe: vLLM Inference Server
# This recipe deploys a vLLM inference server on MeluXina

name: vllm-inference
type: server
description: "vLLM OpenAI-compatible inference server for LLM benchmarking"
version: "1.0.0"

# Container configuration
container:
  image: /project/home/p200776/containers/vllm-latest.sif
  runtime: apptainer
  # Additional bind mounts
  binds:
    - /project/home/p200776/models:/models:ro
    - /dev/shm:/dev/shm

# Resource requirements for SLURM
resources:
  nodes: 1
  gpus: 1
  cpus_per_task: 8
  memory: 64G
  time: "04:00:00"
  partition: gpu

# Network configuration
network:
  ports:
    - name: api
      port: 8000
      protocol: http

# Environment variables
environment:
  MODEL_NAME: "meta-llama/Llama-2-7b-chat-hf"
  TENSOR_PARALLEL_SIZE: "1"
  GPU_MEMORY_UTILIZATION: "0.9"
  MAX_MODEL_LEN: "4096"
  DTYPE: "auto"

# Command to run inside container
command: |
  python -m vllm.entrypoints.openai.api_server \
    --model ${MODEL_NAME} \
    --tensor-parallel-size ${TENSOR_PARALLEL_SIZE} \
    --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION} \
    --max-model-len ${MAX_MODEL_LEN} \
    --dtype ${DTYPE} \
    --host 0.0.0.0 \
    --port 8000

# Health check configuration
healthcheck:
  enabled: true
  endpoint: /health
  port: 8000
  interval: 30
  timeout: 10
  retries: 3
  initial_delay: 60

# Metrics endpoint for monitoring
metrics:
  enabled: true
  endpoint: /metrics
  port: 8000
  type: prometheus

# Labels for organization
labels:
  category: inference
  engine: vllm
  model_type: llm
